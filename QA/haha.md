技术问题？

1. 现在的最新技术根本不是从arxiv上获取，更不是从ACL或者ICML这种回忆是学习。现在都得是Github的trending的python项目、Twitter、机器之心、朋友圈、b站、huggingface daily paper、product hunt、这些渠道，然后才会去慢慢补论文、补技术报告。
2. 买计算卡需要考虑很多方面，但是选择又很少A10，A100，A6000，A800，H800，V100。有人会说H100不是最牛逼的吗，H100别说中国人了，美国人自己都买不到，订单都订到明年的了。NVlink是计算卡之间的通信协议。
3. 大模型训练所需要的显存是推理的3倍，是所占本身大小的4倍。
4. 在时间上，如果是推理，需要GPUflops除以(7B_2_这句话的token数量)，其中几B是指模型参数量，2是指一个参数需要算一次加法和一次乘法共需要算2次。训练所需时间是推理的三倍，前向，求梯度，更新梯度，所以是三倍的关系。所以可以用来粗略地估计一下时间。
5. 不要觉得写prompt没有写代码有技术含量，要考虑到给出多少个合适的example，给不给思维链，写好也很需要耐心。
6. 在prompt1.0时代用bert做基座模型然后构造一些templete是需要计算损失的，但是现在那些基座模型也不在优化了，因此在prompt2.0时代prompt概念已经完全发生变化了，现阶段prompt就是指提示词。
7. 从之前的nlp时代，到如今大模型时代，概念会完善或者变化，预训练=基座模型训练，二次预训练=续训练=全量微调。
8. ReLU激活函数的缺点就是比较radical，因为零点是不可导点，太刚猛，大模型最常用的是GEGLU和SwiGLU，选择这些激活函数的原因其实是根据经验和实际效果试出来的。
9. 代码永远是核心。
10. 大模型数据其实是工作当中最费时间的，算法比较有意思但是部分算法的套路是类似的，算力其实就是钱。
11. sentencepiece是google的分词工具，他可以通过bpe将任何文字转化成utf-8，扩充词表的时候必须要用。
12. 全量微调大模型的时候最好要保证微调数据的token量要大于等于可调参数量，这样才有效果。
13. 待训练的参数，一定要用float32，如果用float16可能会出现nan、inf。同一层的参数一定是同一类型，不同层的参数可以是不同类型。
14. 大模型的所有微调方法都是可以同时使用的，并不是相冲突的。
15. 大模型外挂数据库，是能一定程度上缓解显存不够的问题。
16. Lora中R一般取4，r太大对原有模型破坏越严重，就会产生灾难性遗忘，所以领域越垂直r设置越大，领域越通用r设置越小。
17. 模型转hf格式，是因为有些模型transformer库是不能直接调用的，需要使用额外的转换脚本。
18. 如果数据是非结构化文档，只能用lora去训了。
19. 双精度算力是指FP64，单精度算力是指FP32，半精度算力指FP16，整型算力是指INT8或者INT4这种，这些算力都是可以向下兼容的。一般的算力都是半精度算力(A100,A40,V100,A800)和单精度算力(消费级游戏卡)，很少有双精度和整型算力。
20. wandb是tensorborad的进化版本，不仅图好看，还支持多人协作。
21. nltk是NLP的文本语料处理库。
22. hf格式的大模型有的模型名字后面会标hf，有的不会例如chatglm，但chatglm本身就是，判断是不是hf格式只需要看文件里包不包含这几个文件：*.bin的模型文件，config.json, tokenizer.model, tokenizer_config.json。如果模型后缀是pth那说明不是hf格式的，是huggingface格式的。
23. 预训练语料包括大量的单语数据，平行数据。而指令微调阶段的数据集是指{}中有问答形式的数据，这类型的数据目前占据大模型SFT的主流，指令微调阶段只有output的loss用于梯度回传。
24. 指令微调数据集又分为三种： 增量预训练：只有ouput有内容 单轮对话：input和output都有 多轮对话：多组input和output
25. INF就是上溢出的意思也就是半精度超出65504，如果下溢出就直接变成0变成0就梯度消失了，这个问题该怎么解决呢？最新的显卡用BF16就可以避免。
26. 如何开启半精度训练？在调用from_pretrained这个方法的时候，指定参数torch_dytpe=torch.half即可。
27. 如下：
28. ￼
29. llama的词表比较小，没有针对中文进行专门的训练，所以有时候一个字会有好几个token表示。
30. 半精度不太好把握容易报各种错，单精度好把不会容易出太多错。
31. embedding的作用就是将离散的文本连续化，而且比onehot的好处在于维度更低，空间相关性更好。
32. 单纯做中文分词就可以jieba、spacy、hanlp、bert-base-chinese.sbert-base-chinese。
33. 如果是纯文本数据最好是用向量数据库，但如果是表格类数据还是关系数据库比较合适，如果是非结构化数据最好使用图数据库。
34. 下载pytorch一定注意本地cuda的大版本到底是11还是12，这个11.x 只要和 本地的 nvidia-smi 驱动的 11.x 大版本对上就行，大版本一致的情况下尽可能选择新的pytorch版本。也就是我本地是11.6，conda下载11.7也没有任何关系。
35. spacy是一个自然语言处理库，可以用来做文本预处理、文本解析、命名实体识别、词性标注、句法分析和文本分类等任务。
36. V100不支持bf16
37. .pth和.bin的文件格式，两者都是二进制的格式，一个是torch保存的格式，一个是huggingface的保存格式。safetensor加载更安全更快。
38. 相关int4、int8的量化方法都是基于bitsandbytes这个包实现的。所谓的量化就是使用bitsandbytes.nn.Linear4(8)bit替换模型的所有的nn.Linear。但是不能替换模型的lm_head层。
39. chat模型和非chat模型的区别不是续写，而是输入，chat的输入是一个消息列表里面有角色啥的还有message，而completion模型输入输出就只是一个字符串。
40. tokenizer其实就是训练出一本词典，把词义都解析出来。
41. V100不支持flashattention
