
import os
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_TRACING"] = "false"
os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
os.environ['OPENAI_API_KEY'] = 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
import pandas as pd
from langchain import hub
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ---------- 2) è¯»å–å¹¶æ¸…æ´— Excel ----------
DATA_DIR = Path("/Users/titus.w/Code/MyProfile/GitHub/RAG-Project/p1_rag_tutorial/research-presentation/")
files = ["data1.xlsx", "data2.xlsx", "data3.xlsx"]

dfs = []
for fn in files:
    xl = pd.ExcelFile(DATA_DIR / fn)
    for sheet in xl.sheet_names:
        df = xl.parse(sheet)
        df["__source_file__"] = f"{fn}:{sheet}"
        dfs.append(df)

raw_df = pd.concat(dfs, ignore_index=True).dropna(how="all")
raw_df = raw_df.applymap(lambda x: str(x).strip() if pd.notna(x) else x).drop_duplicates()

docs = [
    Document(
        page_content=f"é—®ï¼š{row['é—®é¢˜']}\nç­”ï¼š{row['ç­”æ¡ˆ']}",
        metadata={"source": row["__source_file__"], "row": int(idx)},
    )
    for idx, row in raw_df.iterrows()
]

# ---------- 3) åˆ‡åˆ† + åµŒå…¥ ----------
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = splitter.split_documents(docs)

embeddings = OpenAIEmbeddings()
vectordb = Chroma.from_documents(splits, embedding=embeddings, persist_directory="stategrid_internal_chroma")
retriever = vectordb.as_retriever(search_kwargs={"k": 4})  # ä¸€æ¬¡å– 4 æ¡ï¼Œæé«˜å›ç­”å®Œæ•´åº¦
print("âœ… å‘é‡æ¡ç›®ï¼š", vectordb._collection.count())

# ---------- 4) Prompt ----------
from langchain.prompts import ChatPromptTemplate
template = """å·²çŸ¥ä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼Œç»“åˆä½ è‡ªå·±çš„çŸ¥è¯†ï¼Œ**ç”¨ä¸­æ–‡**å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
è¦æ±‚ï¼š
- å›ç­”è¦å°½é‡è¯¦ç»†ã€åˆ†æ¡é™ˆè¿°
- æ¯å½“ä½¿ç”¨èµ„æ–™ä¸­çš„ä¿¡æ¯æ—¶ï¼Œåœ¨å¥å­æœ«å°¾ç”¨å¯¹åº”åºå·å¼•ç”¨ï¼Œå¦‚[1]
- æœ€åæ–°å¢ä¸€è¡Œã€Œå¼•ç”¨ï¼šã€æŒ‰åºå·åˆ—å‡ºèµ„æ–™çš„æ¥æº

å‚è€ƒèµ„æ–™ï¼š
{context}

é—®é¢˜ï¼š{question}
å›ç­”ï¼š"""
prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
    """æŠŠæ£€ç´¢ç»“æœç¼–å·ï¼Œä¾› LLM å¼•ç”¨"""
    return "\n\n".join(f"[{i+1}] {doc.page_content}" for i, doc in enumerate(docs))

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.2)
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ---------- 5) äº¤äº’å¼å¯¹è¯ ----------
print("è¾“å…¥é—®é¢˜ï¼ˆexit é€€å‡ºï¼‰ï¼š")
while True:
    user_q = input(">>> ").strip()
    if user_q.lower() == "exit":
        break

    answer = rag_chain.invoke(user_q)
    print("\nğŸ¤– å›ç­”ï¼š\n", answer)

    # å•ç‹¬æ‰“å°æ¥æºæ ‡é¢˜ & ç‰‡æ®µï¼Œæ–¹ä¾¿ç»ˆç«¯æŸ¥çœ‹
    source_docs = retriever.get_relevant_documents(user_q)
    print("\nğŸ”— æ¥æºæ‘˜å½•ï¼š")
    for idx, d in enumerate(source_docs, 1):
        snippet = d.page_content.split("\n")[0][:80]
        print(f"[{idx}] {d.metadata['source']} | {snippet}...")
    print("-" * 70)